{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DataCamp 2020  - PyBullet Reinforcement Learning Workshop - Ugurkan Ates",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "# Bogazici DataCamp 2020 , PyBullet - RL Workshop\n",
        "## Soft Actor Critic algoritması with PyBullet Miniatur ortamında\n",
        "##### Kod yapısı resmi TF Agent egitimlerinden alinmistir.  \n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<img src=\"https://i.ibb.co/S6dw5jd/Screenshot-2020-11-08-at-14-33-30.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## Baslangic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "Bugun ki workshop'ta TF Agents kütüphanesi ve Tensorflow 2.0 kullanarak pyBullet ortamında bulunan bir Reinforcement Learning Agent(Pekiştirmeli Öğrenme Çalışanı/Birimini) eğiteceğiz. \n",
        "\n",
        "Eğitim süresince \n",
        "  - Kurulum ve eğitim sürecini takip etmek\n",
        "  - Eğitim sonrası performansı değerlendirmek\n",
        "  - Colab/Jupiter notebook ortamları üzerinde video şeklinde görüntülemek\n",
        "  - TF Agents yapılarında basitçe RL Agent oluşturabilmek \n",
        "\n",
        "Konularına değineceğiz , eğitim bitiminde bu ortam/notebook link şeklinde size sunulacaktır. Eğitim boyunca not almanız / soru sormanız daha yararlı olabilir birebir kod takibi yerine.\n",
        "\n",
        "Not : Bu kodları  Colab ortamı yerine kişisel bilgisayarınızda da çalıştırabilirsiniz.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUQms4DAY5A"
      },
      "source": [
        "Yüklemeniz gereken paketler :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fskoLlB-AZ9j"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install gym\n",
        "!pip install 'imageio==2.4.0'e\n",
        "!pip install matplotlib\n",
        "!pip install PILLOW\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Kurulum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNV5wyH3dyMl"
      },
      "source": [
        "Bize gerekli olan paketleri import ediyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Alttaki kisimlar TF Agents kutuphanesinde kullanilacak bolumler.\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "# bu algoritmayı kullanmiyoru fakat icindeki critic ag yapisini kullanacagiz \n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.environments import suite_pybullet\n",
        "from tf_agents.experimental.train import actor\n",
        "from tf_agents.experimental.train import learner\n",
        "from tf_agents.experimental.train import triggers\n",
        "\n",
        "# gerekli yardimci paketler / fonksiyonlar\n",
        "from tf_agents.experimental.train.utils import spec_utils\n",
        "from tf_agents.experimental.train.utils import strategy_utils\n",
        "from tf_agents.experimental.train.utils import train_utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "\n",
        "# neural network ile ilgili kisimlar\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "\n",
        "# datalari kullanmak icin - performansli bir veri yapisi\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "\n",
        "tempdir = tempfile.gettempdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparametreler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "source": [
        "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
        "\n",
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment ( Ortam ) \n",
        "\n",
        "Pekistirmeli ogrenmede ortam dedigimiz kavram herhangi bir optimizasyon problemini ifade ediyor. Bu bir gorevin parcasi veya problemin tamamı olabilir. Ornegin robotun parmaklarini kullanarak ( manipule ederek ) dustugu yerden kalkip ziplamasi. Burada birden cok gorev parcasi var ama problem bir adet.\n",
        "\n",
        "TF-Agents paketinin icinde standart bulunan belli basli problemler bulunuyor. Bu problemler RL algoritmalarının ortak zeminde test edilebilmesi icin onem tasiyor.  \n",
        "\n",
        "`suites` adını verdigimiz bu ortam yukleyicilerinin icinde en cok bilinen OpenAI Gym , Atari oyunlari , Deep Mind'in kontrol paketi gibi farklı ortamlar mevcut geliyor. Tek bir string degistirerek farkli ortamı yuklemenizi sagliyor.\n",
        "\n",
        "Bugun ki eğitim icin PyBullet fizik / robotik simulasyonunda bulunan minyatur hayvanı temalı robot ortamını yukleyecegiz.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "source": [
        "env = suite_pybullet.load(env_name)\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY179d1xlmoM"
      },
      "source": [
        "Bu ortamda amac egitecegimiz politika ile minyatur robotun dusmeden ileriye gitmesini saglayabilmek.\n",
        "\n",
        "Herhangi bir deneme(ortam resetlenmeden ileriye gitme sureci) 1000 adım surecek sekilde ayarlandı. Robot 1000 adım durmaksızın gidebilirse ortamı cozmus sayilacaktir. Bu surecte topladigi odullerin toplami kazandigi toplam skor olacaktir.\n",
        "\n",
        "Tabiki dusmesi veya ortam disina cikmasi gibi bir durumda 1000 adım gecmeden ortam resetlenecek sekilde ayarlandi.\n",
        "\n",
        "Simdi egitimde kullanacagimiz ortamın detaylarına goz atalim.\n",
        " `observation` adını verdigimiz bilgiler egitim sırasında gozumuz/kulagimiz olacak ve optimizasyonu bu bilgiler isiginda gerceklestirecegiz.\n",
        " Verilen bilgilere gore politikamız/policy bize ortamda hareket etmemiz gereken / degistirmemiz gereken  `actions` uretecektir.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg5ysVTnctIm"
      },
      "source": [
        "Goruldugu uzere ortamdan gelen \"**bilgiler**\" yeterli seviyede karmasik ve cozumu bulabilmek icin kısa bir egitim yeterli olmayacak. 28 sayısal deger iceriyor bunlar , \n",
        " - Motorun bacak motorlarinin acisal degerleri\n",
        " - Ayak eklemlerinin motor degerleri\n",
        " - Hız degerleri ( acisal ve cizgisel hız) - IMU sensoru\n",
        " - Robotun belirli bolgelerinin tork degerleri.\n",
        "\n",
        "Ful lliste icin https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py\n",
        "\n",
        "Bu degerler icin normalizasyon yapılarak degerler `[-1, 1]` bandına sıkıstiriliyor.\n",
        "\n",
        "Aynı sekilde robotun aksiyonları da `[-1, 1]` araliginda olup 8 adet istenen motor aci degerleridir.\n",
        "\n",
        "Anlasildigi uzere bu sisteme yurutmeyi ogretmek klasik robotik algoritmaları agir muhendislik ve cok fazla guncelleme / test gerektiren bir surec. \n",
        "RL kullanarak aynı algoritma tabanları ile farklı problemleri cozebiliyoruz ve cozulen bu problemler genelde klasik algoritmalardan daha optimal sonuclar veriyor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "source": [
        "collect_env = suite_pybullet.load(env_name)\n",
        "eval_env = suite_pybullet.load(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b45E3UEY1Nr1"
      },
      "source": [
        "Genelde RL sistemlerinde egitim asamasinda iki ortam birlikte yaratilir. Biri veri toplamak icin yapılırken digerinde egitim sonucu ogrenilen network ile degerlendirme / performans saptanır. Bunun sebebi ***bilgi gucu olarak yeterli olmamız / tekrar kurulum ve ayarları yapmamak / veri toplamayı durdurmamak*** gibi sıralanabilir. Bugun ki ortamlar python'da yazılmıs ve numpyarraylerini kullanan kod parcalari olacak. Bu sekilde TF agents'te bulunan Actor Learner API'lerini direk kullanabiliyoruz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da-z2yF66FR9"
      },
      "source": [
        "## Egitim Asamalari Ozeti\n",
        "Egitim Asamalarimiz:\n",
        "* Egitim datasini batchler halinde almak\n",
        "* Tek GPU'da / CPU'da yapiyorsaniz burayi gecin\n",
        "* Coklu GPU'da yapiyorsanız bunları sistemlere dagitmak\n",
        "* Forward Step ( Bilgileri neural networke sokup cikti hesaplatmak)\n",
        "* Algoritmada ki yapıya gore ortalama(mean) loss hesaplamak\n",
        "* Backward Step ( Ters turev islemleri ile gradientleri hesaplayip yeni agirliklari bulmak)\n",
        "\n",
        "İstedigimiz performansı (skoru) elde edene dek bu sekilde devam ediyor olacagiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGREYZCaDB1h"
      },
      "source": [
        "### GPU'da egitim\n",
        "Egitimi GPU'da yapmak istiyorsanız aktif hale getirmeniz gerekiyor.\n",
        "\n",
        "* Edit→Notebook Settings\n",
        "* GPU - > Hardware Accelerator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff5ZZRZI15ds"
      },
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMn5FTs5kHvt"
      },
      "source": [
        "Kullanacaginiz ayara gore ellemeyebilirsiniz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## RL Agent - RL Calisani\n",
        "\n",
        "Bugun egitecegimiz Soft Actor Critic algoritmasını kullanan RL calisani / agent 'ini derin ogrenme aglariyla egitmemiz gerekecek. Problemin karmasikligindan oturu genel gecer bir fonksiyon tahmincisi(approximator) yeterli olmayacaktir ( ornek agac yapilari vs ). \n",
        "\n",
        "Bu algoritma aynı anda egitilen 2 adet derin ogrenme agiyla egitilmektedir. Bunlardan biri **\"Actor\"**  yapısında kullanılacak digeri ise **\"Critic\"** adını verdigimiz yapida olacaktır. \n",
        "\n",
        "Critic adını verdigimiz yapı bize o an bulundugumuz durumda almayı dusundugumuz aksiyonun degerini verecektir `Q(s,a)`.  Bu fonksiyon derin ogrenme agi tarafindan hesaplanmaktadir ( eger problem basit bir sekilde cozulecek sekilde ise herhangi bir 2D matrix yapısında dahi tutabilirdiniz) . Fakat `28 bilgi X  8 aksiyon X (-1,1) sonsuz araligi` icin bu yapi mumkun degildir. \n",
        "\n",
        "Bu sebeple derin ogrenme agina bilgileri girdi/input olarak verip cikti olarak 8 aksiyonu bekleyecegiz. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "# Burdaki neural networklerin yapisi tf agents kutuphanesinde bulunmaktadir\n",
        "# strag scope  datayı nasil dagittiniza ve gpu/cpu sistemlerinize gore ayarlanan\n",
        "# bir parametre , suanlik dikkate almaniza gerek yok.\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None, \n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYy4AH4V7Ph4"
      },
      "source": [
        "Burda egitecegimiz `actor` yapisi ise `criticten` aldigi degerlendirmeyi alarak ve bilgileri kullanarak aksiyon arrayleri uretecektir/tahminleri.\n",
        "\n",
        "Actor'un ag yapisi geregi urettigi sey tan-h squashed distribusyonu olacak olup biz bu distribusyondan `sampling` islemini yapacagiz olasılık distribusyonundan veri elde edecegiz.\n",
        "\n",
        " [Bu konuyla ilgili daha cok bilgi ariyorsaniz](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB5Y3Oub4u7f"
      },
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Elimizdeki ag yapilarini ayaga kaldirip agent/calisan degiskenlerini olusturalim.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer - Veri Tekrarlamak\n",
        "\n",
        "Veri toplama sırasında sakladigimiz yararlı verileri tekrar kullanmamız RL calisanınin ogrenmesi icin cok onemli olacaktir , cunku bu yapıyı kullanmadan her politika/neural network guncellemesi yapildiginda tekrardan veri toplamaniz gerekir. Bunun sebebi PG tabanlı algoritmaların her guncellemede karar mekanizmasını degistirmeleri( DQN tarzi algoritmalarda karar her zaman max olanı almak misal) burda ise her sefer degisiyor.\n",
        "\n",
        "Bu verileri efektif kullanabilmek icin farklı veri yapıları var , bugun gosterecegimiz olan yeni cikmis bir yapı , REVERB ismini alıyor. DeepMind ekibinin gelistirdigi cogu algoritmada rahatlikla kullanabilen bir veri yapısı, detaylarına goz atabilirsiniz.\n",
        "[Reverb](https://deepmind.com/research/open-source/Reverb)\n",
        "\n",
        "\n",
        "Max size bugun icin onemli bir parametre degil , daha cok CPU server clusterlarinda kullanilan asenkron data toplama boyutunu ayarliyor\n",
        "```\n",
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "source": [
        "table_name = 'uniform_table' # uniform yani rastgele veri verebilecek sekilde \n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNvAnkO7JK2"
      },
      "source": [
        "Replay buffer yapımız ortamda olusacak `bilgi,odul,sonraki_bilgi,tamamlanma bilgisi` tarzinda yapilari tensorler olarak saklayacaktir.\n",
        "`tf_agent.collect_data_spec`. -> Detaylı bilgi\n",
        "\n",
        "SAC Algoritmasında(loss hesaplanmasinda) hem suanki bilgi hem sonraki bilgi gerektigi icin veri toplarkende ikili toplamamiz gerekiyor `sequence_length=2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVLUxyUo7HQR"
      },
      "source": [
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "Simdi dataset uretimi yapacagiz. Reverb yapisi kullanip tensorflowun el verdigi fonksiyonlar ile hizlica veri toplama islemi gerceklesecek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Politika - Karar Mekanizmalarımız\n",
        "\n",
        "TF-Agents kutuphanesinde, politikalar RL'de kullanılan politika tanımıyla uyusmaktadir.: Verilmis bir zaman araliginda  `time_step` belirli bir aksiyon veya aksiyon distribüsyonu olusturmaktadir. \n",
        "\n",
        "Kullanılan ana method  `policy_step = policy.step(time_step)`  `policy_step`  -> Zaman Araligi\n",
        "\n",
        "-   `agent.policy` —  Gercek sistemde veya test yaparken kullanacaginiz mekanizma\n",
        "-   `agent.collect_policy` — Sadece data toplamak icin kullanılacak mekanizma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq7JE8IwFe0E"
      },
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_A4rZveEQzW"
      },
      "source": [
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azkJZ8oaF8uc"
      },
      "source": [
        "Politikalar/Karar mekanizmaları her ne kadar RL Calisani/ Agent icinde bulunsalarda onlardan bagimsizda olusturabilirsiniz.\n",
        "\n",
        "Ornegin rastgele hareket secen bir karar mekanizması olusturalim. `tf_agents.policies.random_py_policy`  , her zaman araliginda rastgele bir hareket sececektir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LMqw60Kuso"
      },
      "source": [
        "## Actor Yapısı\n",
        "\n",
        "Actor yapısı politikayla ortam arasindaki iliskileri kontrol eden yapıdır\n",
        "\n",
        "  * Actor icinde bir adet ortamı temsil eden degisken bulundurur (`py_environment`) ve bir adet karar mekanizmasının kopyasını \n",
        "  * Her actor yapısı verilen seri seklindeki veri/dataset icin elinde bulunan karar mekanizmasını kullanır\n",
        "  * Karar mekanizmasına yapilan guncellemeler , egitim kodu icinde `actor.run()` cagirmadan once yapılıyor\n",
        "  * Ortamdan gelen yeni bilgiler veri toplama mekanizmasıyla kaydediliyor ve islem devam ediyor surekli.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjE59ct9fU7W"
      },
      "source": [
        "Actorler calistigi surece (bilgi,aksiyon,odul) mekanizmalarını kaydediyorlar ve reverb sistemi bu numpy arraylerini sonra kullanılmak uzere sakliyor.\n",
        "\n",
        "Bu algoritma icin ikili sekil kaydediyoruz(yukarda bahsedildi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbyGmdiNfNDc"
      },
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaVVC22fOcF"
      },
      "source": [
        "Elimizde suan zeki bir mekanizma olmadigi icin rastgele hareket alan bir yapiyla veri toplamasi yapalım."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGq3SY0kKwsa"
      },
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pkg-0vZP_Ya"
      },
      "source": [
        "Bir tanede birazdan akıllanmaya basladikca daha iyi veriler toplayabilecek olan collect_policy , toplama mekanizması baslatalım. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ooXyk0FZ5j"
      },
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR9CZ-jfPN2T"
      },
      "source": [
        "3.Actor yapımızda algoritma/ag test edilirken kullanılsın ve sonuclari  `actor.eval_metrics(num_eval_episodes)` araciligiyla log etsin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHY2BT5lFhgL"
      },
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6eBGSYiOf83"
      },
      "source": [
        "## Egitim sureci - Learners\n",
        "Learner adı verilen yapı derin ogrenme agi egitimini gerceklestiren yapı. Burada gradient hesaplaması gibi asamalar gerceklesiyor. Kendi basina abstract / capsule edilmis bir yapı detayına girmeden rahatca kullanma imkanı sagliyor. Sakladigimiz veri ve elimizdeki politikayı kullanarak RL calisanının derin ogrenme agi degerlerini guncelliyor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi37YicSFTfF"
      },
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Test - Performans Analizi\n",
        "\n",
        "3.Actorumuz olan test actoru yani ogrenilen karar mekanizmasını test edecek ve performansı ile ilgili bilgi verecek olan yapı  `actor.eval_metrics` fonksiyonu ile bize sonuc verecek\n",
        "Bu tarz bir RL sisteminde genel olarak :\n",
        "\n",
        "* Ortalama Toplam odul miktarı. Karar mekanizmasını bir (max 1000 lik yani episode) seklinde calistirinca toplam aldigi odulun ortalamasi genelde 10 kez olur .\n",
        "* Ortalama bir bolum/episode ne kadar adım suruyor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83iMSHUC71RG"
      },
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnOMvX_eZvOW"
      },
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWURm_rXG-f"
      },
      "source": [
        "Bu modulde daha cok ornek bulunabilir bu tarz sonuc mekanizmaları ile ilgili.\n",
        "[metrics module](https://github.com/tensorflow/agents/blob/master/tf_agents/metrics/tf_metrics.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Egitimi Baslatalim !\n",
        "\n",
        "Asagidaki dongu ortamdan data toplayacak ve aynı sırada toplanildikca egitim yapacak bir python dongusu.\n",
        "\n",
        "Bu sırada belirli aralıklarda test yapılacak ve performans loglanacak (bize bilgi verilecek). Egitim bittikten sonra performans analizini video seklinde izleyecegiz. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pTbJ3PeyF-u"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Sonuc \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Grafikler\n",
        "\n",
        "Ortalama odul miktarini adıma bagli olarak grafige dokelim. Bu sayede robotun 1000 adımlık surecte ne kadar gittigini anlamis olacagiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXKzyGt72HS8"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videolar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "RL sistemleri egitirken onemli bir faktorde egitim suresinde veya sonunda RL calisanini izlemek ve problemi cozup cozemedigini gozle gormektir. Her ne kadar odul alinsada bazen bu istenmeyen sekillerde elde edilebilir. \n",
        "\n",
        "Google Colab'da videoları izletmek icin asagidaki fonksiyon isinizi gorecektir.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Asagidaki kod birkac episodelik test yaparak video kayıt yapacaktır:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgaQN1nXT-h"
      },
      "source": [
        "num_episodes = 3\n",
        "video_filename = 'sac_minitaur.mp4'\n",
        "with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = eval_actor.policy.action(time_step)\n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}